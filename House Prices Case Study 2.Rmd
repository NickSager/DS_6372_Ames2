---
title: "Case Study: House Prices and Regressions"
subtitle: "DS 6372 Project 1"
author: "Anish Bhandari, Will Jones, Nicholas Sager"
date: "6/3/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
# added the line just for testing - Anish Bhandari
# Required Libraries
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggthemes)
library(caret)
library(janitor)
library(doParallel)

#library(e1071)
#library(class)
```


## Introduction

Introduction text

## Data Description

A description of the data, where it came from, and what it contains.

### Read the Data

```{r Import}
train <- read.csv("Data/train.csv")
test <- read.csv("Data/test.csv")

# Merge the data frames and add a column indicating whether they come from the train or test set
train$train <- 1
test$SalePrice <- NA
test$train <- 0
ames <- rbind(train, test)

# Verify data frame
head(ames)
str(ames)
summary(ames)
```
For data cleaning purposes, we will merge test and train into one dataset, keeping in mind that the 1459 NA's in the SalePrice column are from the test set. We will also add a column to indicate whether the row is from the train or test set.

### Data Cleaning
In order to use a linear regression model, we need to convert all of the categorical variables into dummy variables. We will also remove or impute the NA's in the data set. Please see the appendix for details on this process.

First, we will visualize where the NA values are and whether it will affect the first two parameters we are concerned about: Sale Price and Gross Living Area.
```{r}
# Summarize NA's by  column
ames %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  gather(key = "Column", value = "NA_Count", -1) %>%
  filter(NA_Count > 0) %>%
  ggplot(aes(x = reorder(Column, NA_Count), y = NA_Count)) +
  geom_col() +
  coord_flip() +
  theme_gdocs() +
  labs(title = "Number of NA's by Column", x = "Column", y = "NA Count")

# Create a table of the missing NAs by column
ames %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  gather(key = "Column", value = "NA_Count", -1) %>%
  filter(NA_Count > 0) %>%
  arrange(desc(NA_Count)) %>%
  select(-Id) %>% 
  kable()
```
There are not too many NA's in the data set, and they appear mostly to do with lack of a certain feature. For example, if a house does not have a pool, then the PoolQC column will be NA.

```{r NA Values}
# Data Cleaning

# If pool-related variables are NA, assume there is no pool and assign to 0
ames <- ames %>%
  mutate(
    PoolQC = ifelse(is.na(PoolQC), "None", PoolQC),
    PoolArea = ifelse(is.na(PoolArea), 0, PoolArea),
  )
# If garage-related variables are NA, assume there is no garage and assign to 0
ames <- ames %>%
  mutate(
    GarageType = ifelse(is.na(GarageType), "None", GarageType),
    #GarageYrBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt), #These will be changed to the mean because of large year values
    GarageFinish = ifelse(is.na(GarageFinish), "None", GarageFinish),
    GarageCars = ifelse(is.na(GarageCars), 0, GarageCars),
    GarageArea = ifelse(is.na(GarageArea), 0, GarageArea),
    GarageQual = ifelse(is.na(GarageQual), "None", GarageQual),
    GarageCond = ifelse(is.na(GarageCond), "None", GarageCond)
  )
# If Bsmt-related variables are NA, assume there is no Bsmt and assign to 0
ames <- ames %>%
  mutate(
    BsmtQual = ifelse(is.na(BsmtQual), "None", BsmtQual),
    BsmtCond = ifelse(is.na(BsmtCond), "None", BsmtCond),
    BsmtExposure = ifelse(is.na(BsmtExposure), "None", BsmtExposure),
    BsmtFinType1 = ifelse(is.na(BsmtFinType1), "None", BsmtFinType1),
    BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), 0, BsmtFinSF1),
    BsmtFinType2 = ifelse(is.na(BsmtFinType2), "None", BsmtFinType2),
    BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), 0, BsmtFinSF2),
    BsmtUnfSF = ifelse(is.na(BsmtUnfSF), 0, BsmtUnfSF),
    BsmtFullBath = ifelse(is.na(BsmtFullBath), 0, BsmtFullBath),
    BsmtHalfBath = ifelse(is.na(BsmtHalfBath), 0, BsmtHalfBath),
    TotalBsmtSF = ifelse(is.na(TotalBsmtSF), 0, TotalBsmtSF)
  )
# If Fence-related variables are NA, assume there is no Fence and assign to 0
ames <- ames %>%
  mutate(
    Fence = ifelse(is.na(Fence), "None", Fence), 
  )
# If Misc-related variables are NA, assume there is no Misc features and assign to 0
ames <- ames %>%
  mutate(
    MiscFeature = ifelse(is.na(MiscFeature), "None", MiscFeature), 
  )
# If Fireplace-related variables are NA, assume there is no Fireplace and assign to 0
ames <- ames %>%
  mutate(
    FireplaceQu = ifelse(is.na(FireplaceQu), "None", FireplaceQu),
  )
# If Alley-related variables are NA, assume there is no Alley and assign to 0
ames <- ames %>%
  mutate(
    Alley = ifelse(is.na(Alley), "None", Alley),
  )

# Summarize the amount of remaining NA's by column to check what's left
colSums(is.na(ames))

# Use the dummyVars() function to convert categorical variables into dummy variables
# Then use janitor::clean_names() to clean up the column names
dummy_model <- dummyVars(~ ., data = ames)
ames_dummy <- as.data.frame(predict(dummy_model, newdata = ames))
ames_dummy <- clean_names(ames_dummy)

# NOTE: Probably could make the case for deleting NAs here -Nick
# Fill in all remaining na values with the mean of the column
ames_dummy <- ames_dummy %>%
  mutate(across(
    c(-sale_price, -train),
    ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)
  ))
# Summarize the amount of remaining NA's by column
colSums(is.na(ames_dummy))

# Assign ames_dummy to ames for the rest of the analysis
ames <- ames_dummy

# Split the data into training and testing sets
# train <- ames_dummy[ames_dummy$train == 1, ]
# test <- ames_dummy[ames_dummy$train == 0, ] # For Kaggle only. Will split train into train/test for model building.
```
We can write as much or as little about this as we'd like. Basically, we tried to interpret what the NA's actually were saying and then make a more informed guess as to what the were rather than going with the mean. In most cases, this created a new category of "none". Remaining values were imputed to the mean, and the data was split into training and testing sets.

Transformations:
```{r Transformations}
# Create columns for log(SalePrice) and log(GrLivArea)
ames$log_sale_price <- log(ames$sale_price)
ames$log_gr_liv_area <- log(ames$gr_liv_area)

ames$overall_qual_2 = ames$overall_qual^2
ames$lot_area_2 = ames$lot_area^2
ames$log_lot_area = ames$lot_area %>% log()
# ames$year_built_t = plogis(ames_non_dummy$year_built-1940)
ames$log_total_bsmt_sf = ames$total_bsmt_sf %>% log()
ames$log_garage_area = ames$garage_area %>% log()
ames$log_x1st_flr_sf = ames$x1st_flr_sf %>% log()
```
This chunk creates new columns for transformations of various attributes. We will explore the reasoning for these in the Exploratory Data Analysis section below.

## Exploratory Data Analysis
Next, we will start exploring the data for insights into the Ames housing market. This analysis will serve as a foundation for our model building efforts.

Summary Statistics:  
**Fix these boxplots **
```{r Numeric Summary Statistics}
# Summary statistics for all numeric variables
calculate_range <- function(x) {
    return(max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
ames_non_dummy <- ames[sapply(ames, calculate_range) != 1]
summary(ames_non_dummy)

ames_long <- ames_non_dummy %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value")
ames_long %>% ggplot(aes(x = variable, y = value)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free") +
  theme(axis.text.x = element_blank()) +
  labs(title = "Boxplots for each variable", x = "Variables", y = "Values")
```
Here we can see the summary statistics of each numeric variable. Some of the distributions appear to be heavily skewed and will need to be transformed or excluded for use in linear regression. Some variables such as Kitchen above ground, Low Quality square footage, and misc. val have extremely narrow distributions. We will exclude attributes like these from the models as the do not provide much information.

** Fix barplot - too small **
```{r Categorical Summary Statistics}
# Summary statistics for all categorical variables
ames_categorical <- ames[sapply(ames, calculate_range) == 1]
ames_categorical %>%
  summarise_all(mean) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = variable, y = value)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Barplot of categorical variables", x = "Variables", y = "Proportion") +
  coord_flip()
# Fix or exclude the plot.

ames_categorical %>%
  summarise_all(mean) %>%
  round(3) %>%
  t() %>%
  kable(col.names = c("Proportion"), caption = "Proportions of categorical variables")
```
This chart and table show the proportions of the total population which have each categorical variable. Attributes which have a very large or small proportion will likely be excluded from the models as they do not provide additional information. We expect that attributes with moderate proportions and the neighborhood values to be the most useful in predicting sale price.

Next we will plot the a pairs matrix of selected continuous variables to see if there are obvious trends.
```{r}
# GGally plot of selected numeric variables
library(GGally)

lowerFn <- function(data, mapping, method = "lm", ...) {
  p <- ggplot(data = data, mapping = mapping) +
    geom_point(colour = "blue", size = .2) +
    geom_smooth(method = loess, color = "red", ...)
  p
}

# ames_non_dummy %>%
#   ggpairs(lower=list(continuous=lowerFn))

ames_non_dummy %>%
  select(sale_price, gr_liv_area, lot_area, overall_qual, overall_cond,
  year_built, year_remod_add, total_bsmt_sf, garage_area, bedroom_abv_gr, x1st_flr_sf) %>%
  ggpairs(lower=list(continuous=lowerFn))
```
The first relationship that stands out is the one between Gross Living Area and sale price. There is a strong linear relationship here (Corr: 0.70) which may become even stronger with a log transformation. We will consider transformations later on. Lot area also appears to be linearly related to sale price, although with a much lower slope. Overall quality has a strong relationship with sale price as well. This will be another candidate for transformation as the relationship appears quadratic. Surprisingly overall condition does not show a linear relationship with sale price, but the vast majority of overall condition ratings are '5'.
```{r}
summary(as.factor(ames$overall_cond))
```
With scores of '5' removed, this variable may provide useful information about sale price, as there appears to be a linear relationship between price and non-'5' scores. Year Built and Year Remodelled show a relationship with sale price. It is linear at lower values of sale price, but appears quadratic across the whole range. These variables may also be candidates for transformation. Basement area and garage area appear weakly related to price, but contain enough information to include, possibly with transformation. First and second floor area also appear to be related to price. They may be candidates for transformation as well and do show some multicollinearity with Gross Living Area. If these are included, we will have consider whether variance inflation becomes a problem.

```{r}
# Fit a linear model with categorical variables to gauge importance
cbind(ames$sale_price, ames_categorical) %>% #str()
  lm(ames$sale_price ~ ., data = .) %>%
  summary()
```
Here we can see that all of the neighborhood variables are significant, suggesting that neighborhood is an important factor in determining sale price. Several other categorical values look important including: lot_config_fr2, house_style1story, exter_qual_gd, fireplace_qu_none, and sale_type_con.

Looking more closely at the data:

```{r}
# Plot Sale Price vs. Gross Living Area colored by neighborhood, omitting rows where SalePrice is NA
# Convert the dataframe from wide format to long format
ames_long <- ames %>% 
  pivot_longer(
    cols = starts_with("neighborhood_"),
    names_to = "Neighborhood",
    values_to = "value"
  ) %>%
  filter(value == 1) %>%  # Keep only rows where the neighborhood dummy variable is 1
  select(-value)  # Remove the 'value' column as it's no longer needed

ames_long %>%
  filter(!is.na(sale_price)) %>%
  ggplot(aes(x = gr_liv_area, y = sale_price, color = Neighborhood)) +
  geom_point(show.legend = FALSE) +
  theme_gdocs() +
  labs(title = "Sale Price vs. Gross Living Area by Neighborhood", x = "Gross Living Area", y = "Sale Price")
```
There is clearly a relationship between Sale Price and Gross Living Area, and the neighborhoods appear to have a similar but separate relationships. This confirms the significance of Neighborhood that we saw in the linear model above.

We will now look at log transformations of the data to see if there is more linear relationship.
** Display side by side? **
```{r}
# Untransformed variables
par(mfrow = c(1, 2))
ames %>%
  ggplot(aes(x = gr_liv_area, y = sale_price)) +
  geom_point() +
  geom_smooth() +
  theme_gdocs() +
  labs(
    title = "Sale Price vs. Gross Living Area",
    x = "Gross Living Area",
    y = "Sale Price"
  )

# Log Transformed
ames %>%
  ggplot(aes(x = log_gr_liv_area, y = log_sale_price)) +
  geom_point() +
  geom_smooth() +
  theme_gdocs() +
  labs(
    title = "Log(Sale Price) vs. Log(Gross Living Area) by Neighborhood",
    x = "Log(Gross Living Area)",
    y = "Log(Sale Price)"
  )
par(mfrow = c(1, 1))
```
The log transformed sale price and square footage appear to have a more linear relationship. Using these will result in a more accurate regression model at the expense of interpretability.


## Objective 1: Interpretable Regression Model

For this model, we will fit a linear regression with the variables that we have identified as significant. Because the focus of this model is interpretability, we will not include any interaction terms, polynomials, or other transformations.

Based on the exploratory analysis above, we will include the following variables in the regression model: 
- gr_liv_area
- lot_area
- overall_qual
- year_built
- year_remod_add
- total_bsmt_sf
- garage_area
- all neighborhood dummy variables
- lot_config_fr2
- house_style1story
- exter_qual_gd
- fireplace_qu_none
- sale_type_con

```{r}
# Train a linear regression model with caret using CV
train <- ames %>%
  filter(train == 1) %>%
  select(-train)

predictor_vars <- c(
  "gr_liv_area", "lot_area", "overall_qual", "year_built", "year_remod_add",
  "total_bsmt_sf", "garage_area", #"x1st_flr_sf", "x2nd_flr_sf", #removed for vif
  "lot_config_fr2", "house_style1story", "exter_qual_gd", "fireplace_qu_none", "sale_type_con"
) %>% paste(collapse = " + ")
neighborhood_vars <- grep("neighborhood", colnames(train), value = TRUE) %>% paste(collapse = " + ")
terms <- (paste(predictor_vars, neighborhood_vars, sep = " + ", collapse = " + "))
formula <- as.formula(paste("sale_price ~", terms, "- neighborhood_veenker"))

set.seed(137)
ctrl <- trainControl(method = "cv", number = 10, verboseIter = TRUE)
lmFit <- train(formula, data = train, method = "lm", trControl = ctrl, metric = "RMSE")
summary(lmFit)
confint(lmFit$finalModel)

library(car)
vif(lmFit$finalModel)

# Plot the RMSE for each fold
lmFit$resample %>%
  ggplot(aes(x = (1:10), y = RMSE)) +
  geom_point() +
  geom_line() +
  theme_gdocs() +
  labs(title = "RMSE for each fold", x = "Fold", y = "RMSE")
```

Summary table of coefficients.
```{r}
# Summary table of coefficients

# Create a tidy data frame from the model and round the numbers
tidy_fit <- lmFit$finalModel %>%
  broom::tidy() %>%
  mutate(across(where(is.numeric), ~round(., 4)))

# Create a table with bolded rows for p-value < 0.05
table <- tidy_fit %>%
  kable("html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F) %>%
  row_spec(which(tidy_fit$p.value < 0.05), bold = T)

table
```
Note that the reference Neighborhood is Veenker, so all neighborhood adjustments are relative to it.

The interpretable linear regression does moderately well. Its RMSE from 10-fold cross validation on the training data is $34,020. This means that the model is within about $70,000 95% of the time. Given that the mean sale price for a house in Ames during the time period covered by our dataset is $180,000, the RMSE implies that the predicted price is within 38% of the actual price 95% of the time. This is not great, but it is a good starting point. 

The benefit of this type of model is its interpretability. To demonstrate this, we will interpret one numerical coefficient and one categorical coefficient.

- Holding all other variables constant a one hundred square foot increase in gross living area is associated with a $4,940 increase in sale price (p < 0.001 from linear regression). Based on our model, we can be 95% confidence that the true increase in sale price is between $4,350 and $5,530 for a one hundred square foot increase in gross living area.

- Holding all other variables constant, being located in the Old Town neighborhood is associated with a approximately $47,700 decrease in sale price compared to a house in the Veenker neighborhood (p < 0.001 from linear regression). Based on our model, we can be 95% confident that the true decrease in sale price is between $24,450 and $70,984 for a house in the Old Town neighborhood compared to a house in the Veenker neighborhood.

Because we are using a linear regression model, we must check the assumptions of the model:
```{r}
plot(lmFit$finalModel)
```
The residuals for this model show some evidence of non-linearity and non-constant variance (heteroscedasticity). There is no evidence of non-normality, and there are no influential points that need to be addressed. We will address the issues in the next section when including transformations in our model.


## Objective 2: Predictive Model

Summary of approach to include complexity (further EDA if necessary)

Further EDA for Transformations:
```{r}
ames_non_dummy %>%
  select(log_sale_price, log_gr_liv_area, log_lot_area, overall_qual_2, overall_cond,
  year_built, year_remod_add, log_total_bsmt_sf, log_garage_area, bedroom_abv_gr, log_x1st_flr_sf) %>%
  ggpairs(lower=list(continuous=lowerFn))
```

EDA For interactions?:



```{r}
# Glmnet Regression model

# Set up training data
train <- ames %>%
  filter(train == 1) %>%
  select(-train)
test <- ames %>%
  filter(train == 0) %>%
  select(-train)

# Define variables to be used and create formula
predictor_vars <- c(
  "log_gr_liv_area", "log_lot_area", "overall_qual_2", "year_built", "year_remod_add",
  "log_total_bsmt_sf", "log_garage_area", "lot_config_fr2", "house_style1story", "exter_qual_gd",
  "fireplace_qu_none", "sale_type_con"#, "." #Can include "." to make really complex (need to remove saleprice)
) %>%
  paste(collapse = " * ")
neighborhood_vars <- grep("neighborhood", colnames(train), value = TRUE) %>% paste(collapse = " * ")
terms <- (paste(predictor_vars, neighborhood_vars, sep = " + ", collapse = " + "))
formula <- as.formula(paste("log_sale_price ~", terms, "- neighborhood_veenker"))

# Train model
# Check if the model object exists, train if it doesn't
if (file.exists("Models/lm_complex.rds")) {
  # Load the model object from disk
  glmnet.fit <- readRDS("Models/lm_complex.rds")
} else {
  # Perform complex glmnet regression

  # Set up a parallel backend with the number of cores you want to use
  cores <- 8 # Change this to the number of cores you want to use
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)

  set.seed(137)
  fitControl<-trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel = TRUE)
  glmnet.fit<-train(formula,
               data = train,
               method = "glmnet",
               trControl = fitControl
               )
  
  # Stop the parallel backend
  stopCluster(cl)
  
  # Save the model object to disk
  saveRDS(glmnet.fit, "Models/lm_complex.rds")
}

glmnet.fit
plot(glmnet.fit)
opt.pen<-glmnet.fit$finalModel$lambdaOpt #penalty term
coef(glmnet.fit$finalModel,opt.pen)
```

Complex LR with stepwise selection
```{r}
# Stepwise Selection

# Check if the model object exists, train if it doesn't
if (file.exists("Models/lm_stepwise.rds")) {
  # Load the model object from disk
  lmStep <- readRDS("Models/lm_stepwise.rds")
} else {
  # Perform stepwise selection

  # Set up a parallel backend with the number of cores you want to use
  cores <- 8 # Change this to the number of cores you want to use
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)

  set.seed(137)
  lmStep <- train(formula,
    data = train,
    method = "glmStepAIC",
    trControl = trainControl(method = "cv", number = 5, allowParallel = TRUE),
    direction = "both",
    penter = 0.05 # Not Working.
  )
  
  # Stop the parallel backend
  stopCluster(cl)
  
  # Save the model object to disk
  saveRDS(lmStep, "Models/lm_stepwise.rds")
}

summary(lmStep$finalModel)
defaultSummary(data.frame(pred = predict(lmStep), obs = train$log_sale_price))
PRESS(lmStep$finalModel) # Press not working with caret models
varImp(lmStep$finalModel) %>%
  filter(Overall > 4) %>%
  arrange(desc(Overall))

# Output the predictions for the test set to a csv file
# fit4x <- glm(formula = formula(fit4), data = train)
stepwise_pred <- predict(lmStep$finalModel, newdata = test)

stepwise_pred %>%
  data.frame() %>%
  rownames_to_column(var = "id") %>%
  mutate(SalePrice = exp(stepwise_pred)) %>%
  dplyr::select(id, SalePrice) %>%
  write_csv("Predictions/stepwise_predictions.csv")
```

```{r}
# Non-Parametric model

```

Model comparisons / insights.


## Conclusion

Summary of objective 1

Summary and recommendations objective 2

Additional discussion points:  Scope of inference?  What would you do if given more time? Recommendations moving forward? Insight the model gave? Etc.  

## Appendix 

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
