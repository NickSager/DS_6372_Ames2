---
title: "Case Study: House Prices and Regressions"
subtitle: "DS 6372 Project 1"
author: "Anish Bhandari, Will Jones, Nicholas Sager"
date: "6/3/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

# Required Libraries
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggthemes)
library(caret)
library(janitor)
library(doParallel)

#library(e1071)
#library(class)
```


## Introduction

Introduction text

## Data Description

A description of the data, where it came from, and what it contains.

### Read the Data

```{r}
train <- read.csv("Data/train.csv")
test <- read.csv("Data/test.csv")

# Merge the data frames and add a column indicating whether they come from the train or test set
train$train <- 1
test$SalePrice <- NA
test$train <- 0
ames <- rbind(train, test)

# Verify data frame
head(ames)
str(ames)
summary(ames)
```
For data cleaning purposes, we will merge test and train into one dataset, keeping in mind that the 1459 NA's in the SalePrice column are from the test set. We will also add a column to indicate whether the row is from the train or test set.

### Data Cleaning
In order to use a linear regression model, we need to convert all of the categorical variables into dummy variables. We will also remove or impute the NA's in the data set. Please see the appendix for details on this process.

First, we will visualize where the NA values are and whether it will affect the first two parameters we are concerned about: Sale Price and Gross Living Area.
```{r}
# Summarize NA's by  column
ames %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  gather(key = "Column", value = "NA_Count", -1) %>%
  filter(NA_Count > 0) %>%
  ggplot(aes(x = reorder(Column, NA_Count), y = NA_Count)) +
  geom_col() +
  coord_flip() +
  theme_gdocs() +
  labs(title = "Number of NA's by Column", x = "Column", y = "NA Count")

# Create a table of the missing NAs by column
ames %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  gather(key = "Column", value = "NA_Count", -1) %>%
  filter(NA_Count > 0) %>%
  arrange(desc(NA_Count)) %>%
  kable()
```
There are not too many NA's in the data set, and they appear mostly to do with lack of a certain feature. For example, if a house does not have a pool, then the PoolQC column will be NA.

```{r}
# Data Cleaning

# If pool-related variables are NA, assume there is no pool and assign to 0
ames <- ames %>%
  mutate(
    PoolQC = ifelse(is.na(PoolQC), "None", PoolQC),
    PoolArea = ifelse(is.na(PoolArea), 0, PoolArea),
  )
# If garage-related variables are NA, assume there is no garage and assign to 0
ames <- ames %>%
  mutate(
    GarageType = ifelse(is.na(GarageType), "None", GarageType),
    #GarageYrBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt), #These will be changed to the mean because of large year values
    GarageFinish = ifelse(is.na(GarageFinish), "None", GarageFinish),
    GarageCars = ifelse(is.na(GarageCars), 0, GarageCars),
    GarageArea = ifelse(is.na(GarageArea), 0, GarageArea),
    GarageQual = ifelse(is.na(GarageQual), "None", GarageQual),
    GarageCond = ifelse(is.na(GarageCond), "None", GarageCond)
  )
# If Bsmt-related variables are NA, assume there is no Bsmt and assign to 0
ames <- ames %>%
  mutate(
    BsmtQual = ifelse(is.na(BsmtQual), "None", BsmtQual),
    BsmtCond = ifelse(is.na(BsmtCond), "None", BsmtCond),
    BsmtExposure = ifelse(is.na(BsmtExposure), "None", BsmtExposure),
    BsmtFinType1 = ifelse(is.na(BsmtFinType1), "None", BsmtFinType1),
    BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), 0, BsmtFinSF1),
    BsmtFinType2 = ifelse(is.na(BsmtFinType2), "None", BsmtFinType2),
    BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), 0, BsmtFinSF2),
    BsmtUnfSF = ifelse(is.na(BsmtUnfSF), 0, BsmtUnfSF),
    TotalBsmtSF = ifelse(is.na(TotalBsmtSF), 0, TotalBsmtSF)
  )
# If Fence-related variables are NA, assume there is no Fence and assign to 0
ames <- ames %>%
  mutate(
    Fence = ifelse(is.na(Fence), "None", Fence), 
  )
# If Misc-related variables are NA, assume there is no Misc features and assign to 0
ames <- ames %>%
  mutate(
    MiscFeature = ifelse(is.na(MiscFeature), "None", MiscFeature), 
  )
# If Fireplace-related variables are NA, assume there is no Fireplace and assign to 0
ames <- ames %>%
  mutate(
    FireplaceQu = ifelse(is.na(FireplaceQu), "None", FireplaceQu),
  )
# If Alley-related variables are NA, assume there is no Alley and assign to 0
ames <- ames %>%
  mutate(
    Alley = ifelse(is.na(Alley), "None", Alley),
  )

# Summarize the amount of remaining NA's by column to check what's left
colSums(is.na(ames))

# Use the dummyVars() function to convert categorical variables into dummy variables
# Then use janitor::clean_names() to clean up the column names
dummy_model <- dummyVars(~ ., data = ames)
ames_dummy <- as.data.frame(predict(dummy_model, newdata = ames))
ames_dummy <- clean_names(ames_dummy)

# NOTE: Probably could make the case for deleting NAs here -Nick
# Fill in all remaining na values with the mean of the column
ames_dummy <- ames_dummy %>%
  mutate(across(
    c(-sale_price, -train),
    ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)
  ))
# Summarize the amount of remaining NA's by column
colSums(is.na(ames_dummy))

# Assign ames_dummy to ames for the rest of the analysis
ames <- ames_dummy

# Split the data into training and testing sets
# train <- ames_dummy[ames_dummy$train == 1, ]
# test <- ames_dummy[ames_dummy$train == 0, ] # For Kaggle only. Will split train into train/test for model building.
```
We can write as much or as little about this as we'd like. Basically, we tried to interpret what the NA's actually were saying and then make a more informed guess as to what the were rather than going with the mean. In most cases, this created a new category of "none". Remaining values were imputed to the mean, and the data was split into training and testing sets.

Transformations:
```{r}
# Create columns for log(SalePrice) and log(GrLivArea)
ames$log_sale_price <- log(ames$sale_price)
ames$log_gr_liv_area <- log(ames$gr_liv_area)
```

## Exploratory Data Analysis
Next, we will start exploring the data for insights into the Ames housing market. This analysis will serve as a foundation for our model building efforts.

Summary Statistics:
```{r}
# Summary statistics for all numeric variables
calculate_range <- function(x) {
    return(max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
ames_non_dummy <- ames[sapply(ames, calculate_range) != 1]
summary(ames_non_dummy)

ames_long <- ames_non_dummy %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value")
ames_long %>% ggplot(aes(x = variable, y = value)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free") +
  theme(axis.text.x = element_blank()) +
  labs(title = "Boxplots for each variable", x = "Variables", y = "Values")
```

```{r}
# Summary statistics for all categorical variables
ames_categorical <- ames[sapply(ames, calculate_range) == 1]
ames_categorical %>%
  summarise_all(mean) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = variable, y = value)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Barplot of categorical variables", x = "Variables", y = "Proportion") +
  coord_flip()
# Fix or exclude the plot.

ames_categorical %>%
  summarise_all(mean) %>%
  kable()
```

Next we will plot the a pairs matrix of selected continuous variables to see if there are obvious trends.
```{r}
# GGally plot of selected numeric variables
library(GGally)

lowerFn <- function(data, mapping, method = "lm", ...) {
  p <- ggplot(data = data, mapping = mapping) +
    geom_point(colour = "blue", size = .2) +
    geom_smooth(method = loess, color = "red", ...)
  p
}

# ames_non_dummy %>%
#   ggpairs(lower=list(continuous=lowerFn))

ames_non_dummy %>%
  select(log_sale_price, gr_liv_area, lot_area, overall_qual, overall_cond,
  year_built, year_remod_add, total_bsmt_sf, garage_area, bedroom_abv_gr, x1st_flr_sf) %>%
  ggpairs(lower=list(continuous=lowerFn))
```
The first relationship that stands out is the one between Gross Living Area and sale price. There is a strong linear relationship here (Corr: 0.70) which may become even stronger with a log transformation. We will consider transformations later on. Lot area also appears to be linearly related to sale price, although with a much lower slope. Overall quality has a strong relationship with sale price as well. This will be another candidate for transformation as the relationship appears quadratic. Surprisingly overall condition does not show a linear relationship with sale price, but the vast majority of overall condition ratings are '5'.
```{r} summary(as.factor(ames$overall_cond))
```
With scores of '5' removed, this variable may provide useful information about sale price, as there appears to be a linear relationship between price and non-'5' scores. Year Built and Year Remodelled show a relationship with sale price. It is linear at lower values of sale price, but appears quadratic across the whole range. These variables may also be candidates for transformation. Basement area and garage area appear weakly related to price, but contain enough information to include, possibly with transformation. First and second floor area also appear to be related to price. They may be candidates for transformation as well and do show some multicollinearity with Gross Living Area. If these are included, we will have consider whether variance inflation becomes a problem.


```{r}
# Fit a linear model with categorical variables to gauge importance
cbind(ames$sale_price, ames_categorical) %>% #str()
  lm(ames$sale_price ~ ., data = .) %>%
  summary()
```
Here we can see that all of the neighborhood variables are significant, suggesting that neighborhood is an important factor in determining sale price. Several other categorical values look important including: lot_config_fr2, house_style1story, exter_qual_gd, fireplace_qu_none, and sale_type_con.

Looking more closely at the data:

```{r}
# Plot Sale Price vs. Gross Living Area colored by neighborhood, omitting rows where SalePrice is NA
# Convert the dataframe from wide format to long format
ames_long <- ames %>% 
  pivot_longer(
    cols = starts_with("neighborhood_"),
    names_to = "Neighborhood",
    values_to = "value"
  ) %>%
  filter(value == 1) %>%  # Keep only rows where the neighborhood dummy variable is 1
  select(-value)  # Remove the 'value' column as it's no longer needed

ames_long %>%
  filter(!is.na(sale_price)) %>%
  ggplot(aes(x = gr_liv_area, y = sale_price, color = Neighborhood)) +
  geom_point(show.legend = FALSE) +
  theme_gdocs() +
  labs(title = "Sale Price vs. Gross Living Area by Neighborhood", x = "Gross Living Area", y = "Sale Price")
```
There is clearly a relationship between Sale Price and Gross Living Area, and the neighborhoods appear to have a similar but separate relationships. This confirms the significance of Neighborhood that we saw in the linear model above.

We will now look at log transformations of the data to see if there is more linear relationship.
```{r}
# Untransformed variables
ames %>%
  ggplot(aes(x = gr_liv_area, y = sale_price)) +
  geom_point() +
  geom_smooth() +
  theme_gdocs() +
  labs(
    title = "Sale Price vs. Gross Living Area by Neighborhood",
    x = "Gross Living Area",
    y = "Sale Price"
  )

# Log Transformed
ames %>%
  ggplot(aes(x = log_gr_liv_area, y = log_sale_price)) +
  geom_point() +
  geom_smooth() +
  theme_gdocs() +
  labs(
    title = "Log(Sale Price) vs. Log(Gross Living Area) by Neighborhood",
    x = "Log(Gross Living Area)",
    y = "Log(Sale Price)"
  )
```
The log transformed sale price and square footage appear to have a more linear relationship. Using these will result in a more accurate regression model at the expense of interpretability.


## Objective 1: Interpretable Regression Model

For this model, we will fit a linear regression with the variables that we have identified as significant. Because the focus of this model is interpretability, we will not include any interaction terms, polynomials, or other transformations.

Based on the exploratory analysis above, we will include the following variables in the regression model: 
- gr_liv_area
- lot_area
- overall_qual
- year_built
- year_remod_add
- total_bsmt_sf
- garage_area
- all neighborhood dummy variables
- lot_config_fr2
- house_style1story
- exter_qual_gd
- fireplace_qu_none
- sale_type_con

```{r}
# Train a linear regression model with caret using CV
train <- ames %>%
  filter(train == 1) %>%
  select(-train)

predictor_vars <- c(
  "gr_liv_area", "lot_area", "overall_qual", "year_built", "year_remod_add",
  "total_bsmt_sf", "garage_area", #"x1st_flr_sf", "x2nd_flr_sf", #removed for vif
  "lot_config_fr2", "house_style1story", "exter_qual_gd", "fireplace_qu_none", "sale_type_con"
) %>% paste(collapse = " + ")
neighborhood_vars <- grep("neighborhood", colnames(train), value = TRUE) %>% paste(collapse = " + ")
terms <- (paste(predictor_vars, neighborhood_vars, sep = " + ", collapse = " + "))
formula <- as.formula(paste("sale_price ~", terms, "- neighborhood_veenker"))

set.seed(137)
ctrl <- trainControl(method = "cv", number = 10, verboseIter = TRUE)
lmFit <- train(formula, data = train, method = "lm", trControl = ctrl, metric = "RMSE")
summary(lmFit)
confint(lmFit$finalModel)

vif(lmFit$finalModel)

# Plot the RMSE for each fold
lmFit$resample %>%
  ggplot(aes(x = (1:10), y = RMSE)) +
  geom_point() +
  geom_line() +
  theme_gdocs() +
  labs(title = "RMSE for each fold", x = "Fold", y = "RMSE")
```

Summary table of coefficients.

Interpretation of at least one numeric and one categorical.

## Objective 2: Predictive Model

Summary of approach to include complexity (further EDA if necessary)

```{r}
# Regression model

```

```{r}
# Non-Parametric model

```

Model comparisons / insights.


## Conclusion

Summary of objective 1

Summary and recommendations objective 2

Additional discussion points:  Scope of inference?  What would you do if given more time? Recommendations moving forward? Insight the model gave? Etc.  

















## Analysis 1: Sale Price and Gross Living Area

Restate the problem here

### Build and fit the model

#### Entire Dataset

```{r}
# Plot log(Sale Price) vs. log(Gross Living Area) colored by neighborhood, omitting rows where SalePrice is NA
ames %>%
  filter(!is.na(SalePrice)) %>%
  ggplot(aes(x = log(GrLivArea), y = log(SalePrice), color = Neighborhood)) +
  geom_point() +
  theme_gdocs() +
  labs(
    title = "log(Sale Price) vs. log(Gross Living Area) by Neighborhood",
    x = "log(Gross Living Area)",
    y = "log(Sale Price)"
  )
```
This relationship appears to be more linear. We will create columns for the log of Sale Price and Gross Living Area and use these in our analysis.
```{r}
# Create columns for log(SalePrice) and log(GrLivArea)
ames$logSalePrice <- log(ames$SalePrice)
ames$logGrLivArea <- log(ames$GrLivArea)

PRESS <- function(linear.model) {
  #' calculate the predictive residuals
  pr <- residuals(linear.model) / (1 - lm.influence(linear.model)$hat)
  #' calculate the PRESS
  PRESS <- sum(pr^2)

  return(PRESS)
}
# Function for calculating PRESS
# Tom Hopper
# https://gist.github.com/tomhopper/8c204d978c4a0cbcb8c0
```
^Just some code for CV PRESS and log transformations.
Jae: What do you think about using AIC rather than PRESS? The models are optimized using AIC.

#### Century 21 Area

Next, we will visualize the relationship between log Sale Price and log Gross Living Area for the neighborhoods that Century21 operates in: NAmes, Edwards and BrkSide.
```{r}
# Plot log(Sale Price) vs. log(Gross Living Area) colored by neighborhood, omitting rows where SalePrice is NA for only the neighborhoods of interest
century21 <-
  ames %>%
  filter(!is.na(SalePrice)) %>%
  filter(Neighborhood %in% c("NAmes", "Edwards", "BrkSide")) 
century21 %>%
  ggplot(aes(x = logGrLivArea, y = logSalePrice, color = Neighborhood)) +
  geom_point() +
  theme_gdocs() +
  labs(
    title = "log(Sale Price) vs. log(Gross Living Area) by Neighborhood",
    x = "log(Gross Living Area)",
    y = "log(Sale Price)"
  )
```

The relationship appears to be linear, so we will fit a linear model using this data and asses whether it describes the Sale Prices accurately.
```{r}
# Fit a linear model to the data
fit1x <- lm(logSalePrice ~ logGrLivArea + Neighborhood, data = century21)
summary(fit1x)
PRESS(fit1x)

# Fit a linear model to the data with interaction variables
fit1 <- lm(logSalePrice ~ logGrLivArea * Neighborhood, data = century21)
summary(fit1)
PRESS(fit1)
confint(fit1) %>% kable()

# Plot the data with the linear model superposed
century21 %>%
  ggplot(aes(x = logGrLivArea, y = logSalePrice, color = Neighborhood)) +
  geom_point() +
  theme_gdocs() +
  labs(
    title = "log(Sale Price) vs. log(Gross Living Area) by Neighborhood",
    x = "log(Gross Living Area)",
    y = "log(Sale Price)"
  ) +
  geom_smooth(
    method = "lm", formula = y ~ x, se = FALSE, linewidth = 1,
    data = data.frame(
      logGrLivArea = century21$logGrLivArea,
      Neighborhood = century21$Neighborhood,
      logSalePrice = predict(fit1)
    )
  )

# # Print parameter estimate table nicely. Not working, needs debugging
# fit1 %>%
#   summary() %>%
#   {cbind(as.data.frame(coef(.)), .[["coefficients"]][, 2:4])} %>%
#   setNames(c("Estimate", "Std. Error", "t-value", "Pr(>|t|)")) %>%
#   rownames_to_column(var = "Term") %>%
#   mutate(Term = ifelse(Term == "(Intercept)", "Intercept", Term)) %>%
#   add_row(Term = "Adjusted R-squared", Estimate = round(.$adj.r.squared, 3), Std..Error = NA, `t-value` = NA, `Pr(>|t|)` = NA) %>%
#   kable(digits = 3, align = "c") %>%
#   kable_styling(full_width = FALSE)
```
We fit models with and without interaction terms. The interaction terms are statistically significant (include p values) and the model with interaction terms has a lower PRESS. This means that the relationship between log sale price and log gross living area is different for each neighborhood.

It might be good to test whether Edwards and NAmes are are statistically different. We could do that by re-referencing or doing the BYOA method.


### Check the Assumptions

#### Residual Plots
```{r}
# Plot the studentized residuals using base R
plot(fit1$fitted.values, fit1$residuals, type = "p")
plot(fit1$residuals)

# Decide which of these we like better ggplot or R

# Calculate studentized residuals
stud_res <- rstudent(fit1)
# Create a data frame with the studentized residuals
df <- data.frame(stud_res, logGrLivArea = model.frame(fit1)$logGrLivArea)

# Create a scatterplot of the studentized residuals
ggplot(df, aes(x = logGrLivArea, y = stud_res)) +
  geom_point() +
  labs(title = "Scatterplot of Studentized Residuals",
  x = "Studentized Residuals",
  y = "Frequency") +
  theme_minimal()

# Create histogram with normal curve
ggplot(df, aes(x = stud_res)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "lightblue", color = "black") +
  stat_function(fun = dnorm, args = list(mean = mean(df$stud_res), sd = sd(df$stud_res)), color = "blue", size = 1.2) +
  labs(title = "Histogram of Studentized Residuals with Normal Curve",
  x = "Studentized Residuals",
  y = "Frequency") +
  theme_minimal()
```
The plots show the same thing, please include whichever style you think looks better. There is no evidence of non-linearity, heteroscedasticity, or non-normality.

#### Influential Points
```{r}
# Plot the residuals vs. the fitted values
fit1 %>%
  plot()
```
There are no influential points that need to be investigated. The results are consistent with the assumptions of the linear model.

### Comparing competing models

This is already done, just need to move it here. Adj R2, CV press

### Parameters (linear model)

Estimates, interpretation, confidence

### Conclusion

A short summary of the analysis

## RShiny App - Sale price and gross living area

Either embed the app here or link to it

## Analysis 2: Sale Price 

Restate the problem

### Data Cleaning



### Model Selection

talk about feature selection methods

#### Forward Selection
```{r}
# Forward Selection

# # Testing olsrr method.
# library(olsrr)
# fit2x <- lm(log_sale_price ~ . - sale_price, data = train)
# fit2y <- ols_step_forward_p(fit2x, penter = 0.15)$model
# summary(fit2y)
# defaultSummary(data.frame(pred = predict(fit2y), obs = train$log_sale_price))
# PRESS(fit2y)

# Check if the model object exists, train if it doesn't
if (file.exists("Models/lm_forwards.rds")) {
  # Load the model object from disk
  fit2 <- readRDS("Models/lm_forwards.rds")
} else {
  # Perform stepwise selection

  # Set up a parallel backend with the number of cores you want to use
  cores <- 8 # Change this to the number of cores you want to use
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)

  set.seed(137)
  ctrl <- trainControl(
    method = "boot",
    number = 5,
    allowParallel = TRUE
  )
  fit2 <- train(log_sale_price ~ . - sale_price,
    data = train,
    method = "glmStepAIC",
    trControl = ctrl,
    direction = "forward",
    penter = 0.05 # Not Working.
  )

  # Stop the parallel backend
  stopCluster(cl)

  # Save the model object to disk
  saveRDS(fit2, "Models/lm_forwards.rds")
}

summary(fit2$finalModel)
defaultSummary(data.frame(pred = predict(fit2), obs = train$log_sale_price))
PRESS(fit2$finalModel) #Press not working with caret models
varImp(fit2$finalModel)%>%
  filter(Overall > 4) %>%
  arrange(desc(Overall))

# Output the predictions for the test set to a csv file
# fit2x <- glm(formula = formula(fit2), data = train)
forward_pred <- predict(fit2$finalModel, test)

forward_pred %>%
  data.frame() %>%
  rownames_to_column(var = "id") %>%
  mutate(SalePrice = exp(forward_pred)) %>%
  dplyr::select(id, SalePrice) %>%
  write_csv("Predictions/forward_predictions.csv")
```
Forward: AIC -2504, RMSE: .096, R2: 0.942. Adjusted R2 is probably in there, but I'm not sure how to find it.

#### Backward Selection
```{r}
# Backwards Selection

# Check if the model object exists, train if it doesn't
if (file.exists("Models/lm_backwards.rds")) {
  # Load the model object from disk
  fit3 <- readRDS("Models/lm_backwards.rds")
} else {
  # Perform stepwise selection

  # Set up a parallel backend with the number of cores you want to use
  cores <- 8 # Change this to the number of cores you want to use
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)

  set.seed(137)
  fit3 <- train(log_sale_price ~ . - sale_price,
    data = train,
    method = "glmStepAIC",
    trControl = trainControl(method = "cv", number = 5, allowParallel = TRUE),
    direction = "backward",
    penter = 0.05 # Not Working.
  )

  # Stop the parallel backend
  stopCluster(cl)
  
  # Save the model object to disk
  saveRDS(fit3, "Models/lm_backwards.rds")
}

summary(fit3$finalModel)
defaultSummary(data.frame(pred = predict(fit3), obs = train$log_sale_price))
PRESS(fit3$finalModel) # Press not working with caret models
varImp(fit3$finalModel)%>%
  filter(Overall > 4) %>%
  arrange(desc(Overall))


# Output the predictions for the test set to a csv file
# fit3x <- glm(formula = formula(fit3), data = train)
backward_pred <- predict(fit3$finalModel, newdata = test)

backward_pred %>%
  data.frame() %>%
  rownames_to_column(var = "id") %>%
  mutate(SalePrice = exp(backward_pred)) %>%
  dplyr::select(id, SalePrice) %>%
  write_csv("Predictions/backward_predictions.csv")
```
Backward: AIC: -2493, RMSE: .0944, R2: 0.944

#### Stepwise Selection
```{r}
# Stepwise Selection

# Check if the model object exists, train if it doesn't
if (file.exists("Models/lm_stepwise.rds")) {
  # Load the model object from disk
  fit4 <- readRDS("Models/lm_stepwise.rds")
} else {
  # Perform stepwise selection

  # Set up a parallel backend with the number of cores you want to use
  cores <- 8 # Change this to the number of cores you want to use
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)

  set.seed(137)
  fit4 <- train(log_sale_price ~ . - sale_price,
    data = train,
    method = "glmStepAIC",
    trControl = trainControl(method = "cv", number = 5, allowParallel = TRUE),
    direction = "both",
    penter = 0.05 # Not Working.
  )
  
  # Stop the parallel backend
  stopCluster(cl)
  
  # Save the model object to disk
  saveRDS(fit4, "Models/lm_stepwise.rds")
}

summary(fit4$finalModel)
defaultSummary(data.frame(pred = predict(fit4), obs = train$log_sale_price))
PRESS(fit4$finalModel) # Press not working with caret models
varImp(fit4$finalModel) %>%
  filter(Overall > 4) %>%
  arrange(desc(Overall))

# Output the predictions for the test set to a csv file
# fit4x <- glm(formula = formula(fit4), data = train)
stepwise_pred <- predict(fit4$finalModel, newdata = test)

stepwise_pred %>%
  data.frame() %>%
  rownames_to_column(var = "id") %>%
  mutate(SalePrice = exp(stepwise_pred)) %>%
  dplyr::select(id, SalePrice) %>%
  write_csv("Predictions/stepwise_predictions.csv")
```
AIC: -2493, RMSE: .0944, R2: 0.944 (same as backward)

#### Custom features
For custom features, we wil use the top ten parameters ranked by importance from the stepwise model. We expect that this model will not perform as well, but each parameter will be more explainable, and fitting should require much less compute. The question we hope to answer is whether the increase in performance is worth the cost for one of the brute-force models, and whether ten parameters is too few to capture the complexity of the data.
```{r}
# Custom Feature Selection
top10 <- varImp(fit4$finalModel) %>%
  filter(Overall > 4) %>%
  arrange(desc(Overall)) %>%
  head(10) %>%
  rownames()

form <- as.formula(paste("log_sale_price ~", paste(top10, collapse = "+")))
fit5 <- lm(form, data = train)


summary(fit5)
defaultSummary(data.frame(pred = predict(fit5), obs = train$log_sale_price))
PRESS(fit5) # Press not working with caret models
varImp(fit5) %>%
  filter(Overall > 4) %>%
  arrange(desc(Overall))

# Output the predictions for the test set to a csv file
# fit4x <- glm(formula = formula(fit4), data = train)
custom_pred <- predict(fit5, newdata = test)

custom_pred %>%
  data.frame() %>%
  rownames_to_column(var = "id") %>%
  mutate(SalePrice = exp(custom_pred)) %>%
  dplyr::select(id, SalePrice) %>%
  write_csv("Predictions/custom_predictions.csv")
```
AIC: -1190, RMSE: .16, R2: 0.84 (worse but more parsimonious)

### Model Evaluation

Adj R2, Internal CV press, Kaggle score

### Checking Assumptions

Checking the assumptions of the forward model. The other models should be similar.

#### Residual Plots
```{r}
# Plot the studentized residuals using base R
plot(fit2$finalModel$fitted.values, fit2$finalModel$residuals, type = "p")
plot(fit2$finalModel$residuals)
hist(fit2$finalModel$residuals, freq = FALSE)
curve(dnorm(x, mean = mean(fit2$finalModel$residuals), sd = sd(fit2$finalModel$residuals)), add = TRUE, col = "blue")
```

 There is no evidence of non-linearity, heteroscedasticity, or non-normality.

#### Influential Points
```{r}
# Plot the residuals vs. the fitted values
fit2$finalModel %>%
  plot()
```
There are several points with a cook's d approximately 1 which could be investigated. However, the results are consistent with the assumptions of the linear model.

### Conclusion

Conclusion text



## Appendix 

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```